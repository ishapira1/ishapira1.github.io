<div class="project-item">
  <h2 class="project-title">
    <span class="badge-advising">Advising</span>
    Towards Principled AI Alignment: An Evaluation and Augmentation of Inverse Constitutional AI
    <span class="project-year">2025</span>
    <span><a href="https://dash.harvard.edu/handle/1/42719111" class="research-link">[DASH]</a></span>
  </h2>
  <p class="project-description">
    Undergraduate thesis advising for <strong>Esther An</strong> at Harvard SEAS. Topic focused on evaluating and extending inverse constitutional AI for principled alignment.
  </p>
  <hr>
</div>

<div class="project-item">
    <h2 class="project-title">Communication Complexity Bounds for Approximating Voting Rules
      <span class="project-year">2024</span>
      <span><a href="/pdfs/Voting_Rules_Require_Communication.pdf" class="research-link">[PDF]</a></span>
    </h2>
        <p class="project-description">
        Final project for <a href="https://yannai.gonch.name/scientific/syllabi/2023-econ2070-cs237.html" class="course-link">Harvard ECON 2070</a>.
    </p>
    <hr>
</div>

<div class="project-item">
    <h2 class="project-title">Edge of Stability Training Dynamics
        <span class="project-year">2023</span>
        <span><a href="https://sitanchen.com/cs224/f23/assignments/finalproject_ITAI_SHAPIRA_v2.pdf" class="research-link">[PDF]</a></span>
    </h2>
    <p class="project-description">
        Thoughts on EoS and self-stabilizing training dynamics in gradient descent - final project for <a href="https://sitanchen.com/cs224/f23/index.html" class="course-link">Harvard CS224</a>.
    </p>
    <hr>
</div>



<div class="project-item">
    <h2 class="project-title">Contribution to <i>Machine Learning Systems</i>
        <span class="project-year">2023</span>
        <span><a href="https://mlsysbook.ai/" class="research-link">[Website]</a></span>
        <span><a href="https://mlsysbook.ai/Machine-Learning-Systems.pdf" class="research-link">[PDF]</a></span>
    </h2>
    <p class="project-description">
        I contributed to the open-source book <i>Machine Learning Systems</i> by Prof. Vijay Janapa Reddi.
    </p>
    <hr>
</div>


<div class="project-item">
    <h2 class="project-title">Linear Mode Connectivity in Branching SGD Trajectories
      <span class="project-year">2023</span>
      <span><a href="https://github.com/ishapira1/LinearModeConnectivity" class="research-link">[GitHub]</a></span>
      <span><a href="/pdfs/LinearModeConnectivityBranchingTranjectories.pdf" class="research-link">[PDF]</a></span>
    </h2>
    <hr>
<!--    <p class="project-description">This project delved into the loss barrier between two equal-length Stochastic Gradient Descent (SGD) trajectories branching from a common initial portion of training. We studied the linear mode connectivity during training and aimed to understand the behavior of the loss barrier as a function of the number of common gradient steps (k1) and independent steps (k2). Findings revealed that even with an early split in trajectories (low k1), linear mode connectivity persists after many independent steps (k2), and as k2 increases, linear connectivity remains if k1 is sufficiently high, regardless of trajectory divergence in weight-space and prediction-space. We provided evidence suggesting that during the early stages of training, the optimization problem enters an approximately convex basin. These observations were validated across various network architectures and datasets.</p>-->
</div>

<div class="project-item">
    <h2 class="project-title">Accelerating LLMs Decoding Using Random Speculative Sampling Algorithm
      <span class="project-year">2023</span>
      <span><a href="https://github.com/ishapira1/speculative_sampling" class="research-link">[GitHub]</a></span>
    </h2>
    <hr>
<!--    <p class="project-description">In this project, I implemented the Random Speculative Sampling Algorithm as proposed in the paper <a href="https://arxiv.org/abs/2302.01318">"Accelerating Large Language Model Decoding with Speculative Sampling"</a>. The technique speeds up decoding in large language models by parallelizing the decoding process and speculatively predicting multiple tokens at once.</p>-->
</div>

<!--<div class="project-item">-->
<!--    <h2 class="project-title">Indivisible Goods Allocation in Future-Looking Multi-Round Settings-->
<!--      <span class="project-year">2023</span>-->
<!--      <span><a href="/pdfs/Indivisible_Goods_Allocation_Future-Looking_Multi-Round_Settings.pdf" class="research-link">[PDF]</a></span>-->
<!--        <span><a href="/pdfs/IndivisibleGoodsAllocationSlides.pdf" class="research-link">[Slides]</a></span>-->
<!--    </h2>-->
<!--    <hr>-->
<!--    <p class="project-description">Final project of <a href="https://sites.google.com/view/optdemocracy/">Harvard CS-238</a>.-->
<!--&lt;!&ndash;        Analyzing fair mechanisms for allocating indivisible goods in an offline, multi-round setting. This model captures the essence of certain allocation problems, such as resource allocation in food banks, vaccine distribution, and position assignments in organizations. Our main results show it's generally speaking impossible to guarantee fairness without adjustments yet we can bound the number adjustments needed. We construct fair algorithms for two special cases: two-players and binary valuations.&ndash;&gt;-->
<!--    </p>-->
<!--</div>-->

<!--<div class="project-item">-->
<!--    <h2 class="project-title">GiggleGPT - a generative AI Twitter bot-->
<!--      <span class="project-year">2023</span>-->
<!--      <span><a href="https://twitter.com/GiggleGpt" class="research-link">[Twitter]</a></span>-->
<!--    </h2>-->
<!--    <hr>-->
<!--    <p class="project-description">GiggleGPT is a generative AI Twitter bot that reads news articles featuring human achievements, mishaps, and quirks, and crafts tweets to its audience. It continually improves its comedic prowess by analyzing its success based on the number of likes each tweet receives, allowing it to fine-tune its humor.</p>-->
<!--</div>-->
